import streamlit as st
import os
import io  # â† `BytesIO` ã‚’ä½¿ã†ãŸã‚ã«è¿½åŠ 
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.graph_objs as go
from sklearn.cluster import KMeans
from pdfminer.high_level import extract_text
from PIL import Image, ImageEnhance, ImageDraw, ImageFont
from datetime import datetime, timedelta
import time
import hashlib
from auth import check_password



# ãƒšãƒ¼ã‚¸ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š
st.set_page_config(
    page_title="Patent Analysis App.",
    page_icon="ğŸ§Š",
    layout="wide",
    initial_sidebar_state="expanded",
    )

# Pillowã®DecompressionBombErrorã®ãƒã‚§ãƒƒã‚¯ã‚’ç„¡åŠ¹ã«ã™ã‚‹
Image.MAX_IMAGE_PIXELS = None


# ãƒšãƒ¼ã‚¸é¸æŠãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¡¨ç¤º
st.sidebar.title("Navigation window")

pi = 3.14159265359
page_list = ["Home", "Patent", "Claim", "Others"]
analysis_list = ["Overview", "Applicant", "FI", "Summary"]
margin = 1.1
OF = -0.93
cms = plt.cm.datad.keys()

page = st.sidebar.selectbox("Select measurements for analysis.", page_list)

# FIåˆ†é¡ã‚’æ•´ç†ã—ã€æ•°å€¤ã®ã¿ã®ã‚¨ãƒ³ãƒˆãƒªã‚’å‰ã®FIã‚³ãƒ¼ãƒ‰ã¨çµåˆ
def merge_fi_codes(fi_list):
    merged_list = []
    prev_fi = None

    for fi in fi_list:
        if fi.split('@')[0].isdigit() and prev_fi:  # æ•°å­—ã®ã¿ã®å ´åˆã€å‰ã®FIã¨çµåˆ
            merged_list[-1] = f"{prev_fi}-{fi}"
        else:
            merged_list.append(fi)
            prev_fi = fi  # ç›´å‰ã®FIã‚³ãƒ¼ãƒ‰ã‚’æ›´æ–°

    return merged_list

# FIã‚³ãƒ¼ãƒ‰ã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€ã‚¯ãƒ©ã‚¹ã€ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã€ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†è§£ã™ã‚‹é–¢æ•°
def parse_fi_codes(fi_list):
    sections = set()
    classes = set()
    subclasses = set()
    groups = set()

    for fi in fi_list:
        parts = fi.split("/")  # FIã‚³ãƒ¼ãƒ‰ã®"/"ã§åˆ†å‰²
        if len(parts) >= 2:
            main_part = parts[0]  # ä¾‹: "H01L21"
            section = main_part[0]  # ä¾‹: "H"
            subclass = main_part[:4]  # ä¾‹: "H01L"

            sections.add(section)
            subclasses.add(subclass)

            if len(main_part) >= 3:
                class_code = main_part[:3]  # ä¾‹: "H01"
                classes.add(class_code)

            groups.add(fi.split('-')[0].split('@')[0])  # ã‚°ãƒ«ãƒ¼ãƒ—ã¯å…ƒã®FIã‚³ãƒ¼ãƒ‰ãã®ã¾ã¾

    return list(sections), list(classes), list(subclasses), list(groups)

# ğŸ“Œ PDF ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹é–¢æ•°
@st.cache_data
def extract_text_from_pdf(file_bytes):
    """PDF ã®ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹"""
    file_hash = hashlib.md5(file_bytes).hexdigest()  # ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’å–å¾—
    with io.BytesIO(file_bytes) as pdf_file:  # `BytesIO` ã‚’ä½¿ã£ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåŒ–
        extracted_text = extract_text(pdf_file).replace(' ', '').replace('\n', '').replace('\u3000', '')
    return extracted_text, file_hash

# ğŸ“Œ ç„¡é™ã«è‰²ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ï¼ˆHSLã‚’ä½¿ã£ã¦è‡ªå‹•ç”Ÿæˆï¼‰
def generate_color(index):
    hue = (index * 137.508) % 360  # é»„é‡‘æ¯”ã‚’ä½¿ã£ã¦è‰²ç›¸ã‚’å‡ç­‰ã«åˆ†å¸ƒ
    return f"hsl({hue}, 75%, 75%)"

# ğŸ“Œ ãƒ†ã‚­ã‚¹ãƒˆã‚’HTMLå½¢å¼ã§ãƒã‚¤ãƒ©ã‚¤ãƒˆã™ã‚‹é–¢æ•°
def highlight_text(text, terms):
    for i, term in enumerate(terms):
        color = generate_color(i)  # å‹•çš„ã«è‰²ã‚’æ±ºå®š
        text = re.sub(f"({re.escape(term)})", rf'<mark style="background-color: {color}">\1</mark>', text, flags=re.IGNORECASE)
    return text

# å„ãƒšãƒ¼ã‚¸ã®å†…å®¹
# Home
if page==page_list[0]:
    st.title(page_list[0])
    st.title("Patent Analysis App")
    st.write("Welcome to the Patent Analysis Application. Please select an analysis type from the sidebar.")

# Patent
elif page==page_list[1]:
    st.title(page_list[1])
    target_col = 'Patent'
    file_summaries = st.file_uploader("Upload CSV files", type='csv', accept_multiple_files=True)
    if len(file_summaries)>0:
        df_list = []
        for file_summary in file_summaries:
            df_summary = pd.read_csv(file_summary, encoding='utf-8', encoding_errors='ignore')
            df_list.append(df_summary)
        df = pd.concat(df_list).reset_index(drop=True)
        df = df.drop_duplicates(subset=['æ–‡çŒ®ç•ªå·','å‡ºé¡˜æ—¥'], keep='first').reset_index(drop=True)
        target_date_col = st.sidebar.selectbox("Select date column", df.columns.to_list(), index=2)
        df[target_date_col] = pd.to_datetime(df[target_date_col])
        df['å¹´'] = df[target_date_col].dt.year.astype('int')
        df['å‡ºé¡˜äºº/æ¨©åˆ©è€…'] = df['å‡ºé¡˜äºº/æ¨©åˆ©è€…'].astype('str')
        df['FI'] = df['FI'].astype('str').apply(lambda x: [fi for fi in x.split(',')])
        df['FI'] = df['FI'].apply(merge_fi_codes)
        df[['ã‚»ã‚¯ã‚·ãƒ§ãƒ³', 'ã‚¯ãƒ©ã‚¹', 'ã‚µãƒ–ã‚¯ãƒ©ã‚¹', 'ã‚°ãƒ«ãƒ¼ãƒ—']] = df['FI'].apply(lambda x: pd.Series(parse_fi_codes(x)))

        applicant_list =[]
        for applicants in df['å‡ºé¡˜äºº/æ¨©åˆ©è€…'].unique():
            applicants = applicants.replace('ã€',',')
            applicants = applicants.replace('ï¼Œ',',')
            applicants = applicants.replace(', ',',')
            applicants = applicants.replace(',ã€€',',')
            applicants_split = str(applicants).split(',')
            for applicant in applicants_split:
                if applicant not in applicant_list:
                    if len(applicant)>1:
                        applicant_list.append(applicant)
        applicant_list.sort()
        
        # pandas.Timestamp â†’ datetime.date ã«å¤‰æ›
        min_date = df[target_date_col].min().date()
        max_date = df[target_date_col].max().date()
        
        start_date = st.sidebar.date_input("Start date", min_date, min_value=min_date, max_value=max_date)
        end_date = st.sidebar.date_input("End date", max_date, min_value=min_date, max_value=max_date)

        df_date = df[(df[target_date_col]>=str(start_date))&(df[target_date_col]<=str(end_date))]

        min_year = df_date['å¹´'].min()
        max_year = df_date['å¹´'].max()
        df_year = pd.DataFrame(range(min_year,max_year), columns=['å¹´'])
        df_year['count'] = 0
        for year in range(min_year,max_year):
            df_year.loc[df_year['å¹´']==year, 'count'] = df_date[df_date['å¹´']==year]['æ–‡çŒ®ç•ªå·'].count()

        stage_selector = st.sidebar.multiselect("Select stage type", df['ã‚¹ãƒ†ãƒ¼ã‚¸'].unique(), default=df['ã‚¹ãƒ†ãƒ¼ã‚¸'].unique(), key='stage_selector')
        for stage in stage_selector:
            for year in range(min_year,max_year):
                df_year.loc[df_year['å¹´']==year, f'count_{stage}'] = df_date[(df_date['å¹´']==year)&(df_date['ã‚¹ãƒ†ãƒ¼ã‚¸']==stage)]['æ–‡çŒ®ç•ªå·'].count()

        fi_selector = st.sidebar.multiselect("Select FI Section code", df['ã‚»ã‚¯ã‚·ãƒ§ãƒ³'].explode().unique(), default=df['ã‚»ã‚¯ã‚·ãƒ§ãƒ³'].explode().unique(), key='fi_section_selector')
        df_fi = df_date[df_date['ã‚»ã‚¯ã‚·ãƒ§ãƒ³'].apply(lambda x: any(fi in x for fi in fi_selector))]

        with st.spinner('Loading...'):
            # å‡ºé¡˜äººã”ã¨ã®ä»¶æ•°ã‚’è¨ˆç®—
            df_applicant_initial = df_date.copy()
            df_applicant_initial['å‡ºé¡˜äºº/æ¨©åˆ©è€…'] = df_applicant_initial['å‡ºé¡˜äºº/æ¨©åˆ©è€…'].str.split('[ã€ï¼Œ,]')  # å‡ºé¡˜äººåã‚’åˆ†å‰²
            df_applicant_initial = df_applicant_initial.explode('å‡ºé¡˜äºº/æ¨©åˆ©è€…')  # å‡ºé¡˜äººåã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å±•é–‹
            # ç·ä»¶æ•°ã‚’é›†è¨ˆ
            df_applicant_grouped = df_applicant_initial.groupby('å‡ºé¡˜äºº/æ¨©åˆ©è€…')['æ–‡çŒ®ç•ªå·'].count().reset_index(drop=False)
            df_applicant_grouped.rename(columns={'æ–‡çŒ®ç•ªå·': 'ä»¶æ•°'}, inplace=True)
            # ã‚¹ãƒ†ãƒ¼ã‚¸ã”ã¨ã®ä»¶æ•°ã‚’é›†è¨ˆ
            df_stage_grouped = df_applicant_initial.groupby(['å‡ºé¡˜äºº/æ¨©åˆ©è€…', 'ã‚¹ãƒ†ãƒ¼ã‚¸'])['æ–‡çŒ®ç•ªå·'].count().unstack(fill_value=0)
            df_stage_grouped.columns = [f'ã‚¹ãƒ†ãƒ¼ã‚¸_{stage}' for stage in df_stage_grouped.columns]  # åˆ—åã‚’å¤‰æ›´
            df_stage_grouped.reset_index(drop=False, inplace=True)
            # å¹´ã”ã¨ã®ä»¶æ•°ã‚’é›†è¨ˆ
            df_year_grouped = df_applicant_initial.groupby(['å‡ºé¡˜äºº/æ¨©åˆ©è€…', 'å¹´'])['æ–‡çŒ®ç•ªå·'].count().unstack(fill_value=0)
            df_year_grouped.columns = [f"{year}å¹´" for year in df_year_grouped.columns]  # åˆ—åã‚’å¤‰æ›´
            df_year_grouped.reset_index(drop=False, inplace=True)
            # ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
            df_applicant = df_applicant_grouped.merge(df_stage_grouped, on='å‡ºé¡˜äºº/æ¨©åˆ©è€…', how='left')
            df_applicant = df_applicant.merge(df_year_grouped, on='å‡ºé¡˜äºº/æ¨©åˆ©è€…', how='left')
            # å‡ºé¡˜äººã®ä»¶æ•°é †ã«ã‚½ãƒ¼ãƒˆ
            df_applicant.sort_values('ä»¶æ•°', ascending=False, inplace=True)
        
        

        applicant = st.sidebar.selectbox("Select applicant", df_applicant['å‡ºé¡˜äºº/æ¨©åˆ©è€…'].unique(), index=0)

        tab_overview, tab_applicant, tab_fi, tab_summary = st.tabs(analysis_list)

        with tab_overview:
            st.header(analysis_list[0])
            st.write("This is an overview analysis page.")
            st.write("Please select the date range you want to analyze.")
            st.write("The selected date range is from {} to {}.".format(start_date, end_date))
            st.write("The data is as follows.")

            # ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
            st.write(df_date)

            # ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–
            st.header("Visualization")
            with st.spinner('Visualizing...'):
                fig1 = go.Figure()
                fig1.update_layout(title='Patents every year', xaxis_title='Year', yaxis_title='Counts')
                fig1.add_trace(go.Scatter(
                    x=df_year['å¹´'].values, 
                    y=df_year['count'].values, 
                    mode='lines+markers', 
                    name='Counts every year (All)'
                    ))
                for stage in stage_selector:
                    fig1.add_trace(go.Scatter(
                        x=df_year['å¹´'].values, 
                        y=df_year[f'count_{stage}'].values, 
                        mode='lines+markers', 
                        name=f'Counts every year ({stage})'
                        ))
                st.plotly_chart(fig1)

        with tab_applicant:
            st.header(analysis_list[1])
            st.write("This is an applicant analysis page.")

            df_applicant.sort_values('ä»¶æ•°', ascending=False, inplace=True)

            # ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
            st.write(df_applicant)

            num_applicant = st.slider("Number of applicants", 1, len(df_applicant), 50)

            # ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–
            st.header("Visualization")
            with st.spinner('Visualizing...'):
                fig2 = go.Figure()
                # ã‚¹ãƒ†ãƒ¼ã‚¸ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã€å„ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’ç©ã¿ä¸Šã’æ£’ã‚°ãƒ©ãƒ•ã«ã™ã‚‹
                for stage in df_applicant.columns:
                    if stage.startswith("ã‚¹ãƒ†ãƒ¼ã‚¸_"):  # ã‚¹ãƒ†ãƒ¼ã‚¸é–¢é€£ã®ã‚«ãƒ©ãƒ ã®ã¿ã‚’å¯¾è±¡
                        fig2.add_trace(go.Bar(
                            x=df_applicant[stage].values[:num_applicant],
                            y=df_applicant['å‡ºé¡˜äºº/æ¨©åˆ©è€…'].values[:num_applicant],
                            name=stage.replace("ã‚¹ãƒ†ãƒ¼ã‚¸_", ""),  # ãƒ©ãƒ™ãƒ«ã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«
                            orientation='h'
                            ))
                fig2.update_layout(
                    title='Patents per Applicant',
                    height=1200,
                    width=900,
                    xaxis_title='Counts',
                    yaxis_title='Applicants',
                    yaxis=dict(autorange="reversed"),  # ä»¶æ•°ãŒå¤šã„é †ã«ä¸Šã‹ã‚‰è¡¨ç¤º
                    barmode='stack'  # ç©ã¿ä¸Šã’æ£’ã‚°ãƒ©ãƒ•
                    )
                st.plotly_chart(fig2)

            with st.spinner("Visualizing..."):
                fig3 = go.Figure()
                fig3.add_trace(go.Scatter(
                    x=df_year_grouped.columns.to_list()[1:], 
                    y=df_year_grouped[df_year_grouped['å‡ºé¡˜äºº/æ¨©åˆ©è€…'].str.contains(applicant)].T.iloc[:,0].to_list()[1:],
                    mode='lines+markers',
                    name='Patents per Stage'
                    ))
                fig3.update_layout(
                    title=f'Patents per Applicant ({applicant})',
                    xaxis_title='Year',
                    yaxis_title='Counts'
                    )
                st.plotly_chart(fig3)

        with tab_fi:
            st.header(analysis_list[2])
            st.write("This is a FI analysis page.")
            fi_reference_url = 'https://www.j-platpat.inpit.go.jp/cache/classify/patent/PMGS_HTML/jpp/FI/ja/fiSection/fiSection.html'
            st.write(f"Please refer to the following URL for the FI classification: [J-PlatPat:FIã‚»ã‚¯ã‚·ãƒ§ãƒ³/åºƒåŸŸãƒ•ã‚¡ã‚»ãƒƒãƒˆé¸æŠğŸ“Œ]({fi_reference_url})")

            fig4 = go.Figure()
            fig4.add_trace(go.Pie(
                labels=df_fi['ã‚»ã‚¯ã‚·ãƒ§ãƒ³'].explode().value_counts().index,
                values=df_fi['ã‚»ã‚¯ã‚·ãƒ§ãƒ³'].explode().value_counts().values,
                rotation=0,
                hole=0.3,
                title='FI Section',
                textinfo='label+percent',
                ))
            fig4.update_layout(
                title='FI Section',
                height=800,
                width=800,
                )
            st.plotly_chart(fig4)

            fig5 = go.Figure()
            fig5.add_trace(go.Bar(
                x=df_fi['ã‚¯ãƒ©ã‚¹'].explode().value_counts().index,
                y=df_fi['ã‚¯ãƒ©ã‚¹'].explode().value_counts().values,
                name='FI Class',
                ))
            fig5.update_layout(
                title='FI Class',
                height=600,
                width=1200,
                xaxis_title='FI Class',
                yaxis_title='Counts',
                )
            st.plotly_chart(fig5)

        with tab_summary:
            st.header(analysis_list[3])
            st.write("This is a summary analysis page.")
            if 'è¦ç´„' not in df.columns.to_list():
                st.write("There is no summary data in the uploaded file.")
            else:
                st.write(df_date[['æ–‡çŒ®ç•ªå·','å‡ºé¡˜äºº/æ¨©åˆ©è€…','è¦ç´„']])



# Claim
elif page==page_list[2]:
    st.title(page_list[2])
    target_col = 'Claim'
    file_pdfs = st.file_uploader("Upload PDF files", type='pdf', accept_multiple_files=True)

    total_files = len(file_pdfs)  # å…¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°

    pdf_name_list = []
    pdf_text_dict = {}  # ãƒãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã™ã‚‹è¾æ›¸

    if len(file_pdfs)>0:
        extract_bar = st.progress(0)  # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®è¿½åŠ 
        with st.spinner('Loading...'):
            for i, file_pdf in enumerate(file_pdfs):
                # ğŸ“Œ ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
                file_bytes = file_pdf.read()
                extracted_text, file_hash = extract_text_from_pdf(file_bytes)
                pdf_name_list.append(file_pdf.name)
                pdf_text_dict[file_hash] = extracted_text  # ãƒãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã§ä¿å­˜

                extract_bar.progress((i+1)/total_files, f"Extracting {i+1}/{total_files}")
                time.sleep(0.2)

        extract_bar.empty()  # ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ãŸã‚‰ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’æ¶ˆã™

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ã‚’è¿½åŠ ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°å…¥åŠ›ï¼‰
    search_query = st.sidebar.text_input("Enter keywords (comma separated)", "")
    # æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›ï¼ˆã‚«ãƒ³ãƒã§åˆ†å‰²ã—ã¦å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤ï¼‰
    search_terms = [term.strip() for term in search_query.split(',') if term.strip()]

    if len(file_pdfs)>0:
        display_bar = st.progress(0)  # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®è¿½åŠ 
        st.write("Search terms: ", search_terms)
        with st.spinner('Loading...'):
            for i, (name, text) in enumerate(zip(pdf_name_list, pdf_text_dict.values())):
                text = text.replace(' ','')
                st.header(f"{i+1}/{total_files}: {name}")  # ãƒ•ã‚¡ã‚¤ãƒ«åã®è¡¨ç¤º
                try:
                    subject = text.split('ã€èª²é¡Œã€‘')[1].split('ã€è§£æ±ºæ‰‹æ®µã€‘')[0]
                    subject = subject.replace('\n','')
                    subject = subject.replace('\u3000','')
                    subject = highlight_text(subject, search_terms)  # ğŸ” ãƒã‚¤ãƒ©ã‚¤ãƒˆå‡¦ç†
                    st.markdown(f'**ã€èª²é¡Œã€‘**<br>{subject}', unsafe_allow_html=True)
                    # st.write('ã€èª²é¡Œã€‘'+subject)
                except:
                    pass
                try:
                    solution = text.split('ã€è§£æ±ºæ‰‹æ®µã€‘')[1].split('ã€é¸æŠå›³ã€‘')[0]
                    solution = solution.replace('\n','')
                    solution = solution.replace('\u3000','')
                    solution = highlight_text(solution, search_terms)  # ğŸ” ãƒã‚¤ãƒ©ã‚¤ãƒˆå‡¦ç†
                    st.markdown(f'**ã€è§£æ±ºæ‰‹æ®µã€‘**<br>{solution}', unsafe_allow_html=True)
                    # st.write('ã€è§£æ±ºæ‰‹æ®µã€‘'+solution)
                except:
                    pass
                try:
                    figure = text.split('ã€é¸æŠå›³ã€‘')[1].split('ã€ç‰¹è¨±è«‹æ±‚ã®ç¯„å›²ã€‘')[0]
                    figure = figure.replace('\n','')
                    figure = figure.replace('\u3000','')
                    st.write('ã€é¸æŠå›³ã€‘'+figure)
                except:
                    pass
                try:
                    claims = text.split('ã€ç‰¹è¨±è«‹æ±‚ã®ç¯„å›²ã€‘')[1].split('ã€ç™ºæ˜ã®è©³ç´°ãªèª¬æ˜ã€‘')[0]
                    claims = claims.replace('\n','')
                    claims = claims.replace('\u3000','')
                    claims_list = claims.split('ã€è«‹æ±‚é …')
                    for claim in claims_list:
                        if claim != '': 
                            claim_text = highlight_text('ã€è«‹æ±‚é …' + claim, search_terms)  # ğŸ” ãƒã‚¤ãƒ©ã‚¤ãƒˆå‡¦ç†
                            st.markdown(claim_text, unsafe_allow_html=True)
                            # st.write('ã€è«‹æ±‚é …'+claim)
                except:
                    pass

                # ğŸ“Œ ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’æ›´æ–°
                display_bar.progress((i+1)/total_files, f"Processing {i+1}/{total_files}")
                # ğŸ“Œ å°‘ã—å¾…æ©Ÿï¼ˆè¦‹ã‚„ã™ãã™ã‚‹ãŸã‚ï¼‰
                time.sleep(0.2)

        display_bar.empty()  # ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ãŸã‚‰ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’æ¶ˆã™

        st.header("EOF")


# Others
elif page==page_list[3]:
    st.title(page_list[3])
    st.write("This is a page for other analysis.")