import streamlit as st
import os
import io
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.graph_objs as go
from sklearn.cluster import KMeans
from pdfminer.high_level import extract_text
from PIL import Image, ImageEnhance, ImageDraw, ImageFont
from datetime import datetime, timedelta
import time
import hashlib

# --- èªè¨¼æ©Ÿèƒ½ ---
# auth.py ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯èªè¨¼ã‚’å®Ÿæ–½ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯è­¦å‘Šè¡¨ç¤ºï¼‰
# try:
#     from auth import check_password
#     if not check_password():
#          st.error("èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
#          st.stop()
# except ImportError:
#     st.warning("èªè¨¼æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")

# --- ãƒšãƒ¼ã‚¸ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š ---
st.set_page_config(
    page_title="Patent Analysis App.",
    page_icon="ğŸ§Š",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Pillowã®DecompressionBombErrorã®ãƒã‚§ãƒƒã‚¯ã‚’ç„¡åŠ¹åŒ–ï¼ˆå¤§ããªç”»åƒã«ã¯æ³¨æ„ï¼‰
Image.MAX_IMAGE_PIXELS = None

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šãƒšãƒ¼ã‚¸é¸æŠ ---
st.sidebar.title("Navigation window")
page_list = ["Home", "Patent", "Claim", "Others"]
analysis_list = ["Overview", "Applicant", "FI", "Summary"]
page = st.sidebar.selectbox("Select measurements for analysis.", page_list)

# --- FIã‚³ãƒ¼ãƒ‰å‡¦ç†é–¢é€£ ---
def merge_fi_codes(fi_list):
    """FIã‚³ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆä¸­ã€æ•°å­—ã®ã¿ã®ã‚¨ãƒ³ãƒˆãƒªã¯ç›´å‰ã®ã‚³ãƒ¼ãƒ‰ã¨çµåˆã™ã‚‹"""
    merged_list = []
    prev_fi = None
    for fi in fi_list:
        if fi.split('@')[0].isdigit() and prev_fi:
            merged_list[-1] = f"{prev_fi}-{fi}"
        else:
            merged_list.append(fi)
            prev_fi = fi
    return merged_list

def parse_fi_codes(fi_list):
    """FIã‚³ãƒ¼ãƒ‰ã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€ã‚¯ãƒ©ã‚¹ã€ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã€ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†è§£ã™ã‚‹"""
    sections, classes, subclasses, groups = set(), set(), set(), set()
    for fi in fi_list:
        parts = fi.split("/")
        if len(parts) >= 2:
            main_part = parts[0]
            if len(main_part) >= 1:
                sections.add(main_part[0])
            if len(main_part) >= 4:
                subclasses.add(main_part[:4])
            if len(main_part) >= 3:
                classes.add(main_part[:3])
            groups.add(fi.split('-')[0].split('@')[0])
    return list(sections), list(classes), list(subclasses), list(groups)

# --- PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰ ---
@st.cache_data
def extract_text_from_pdf(file_bytes):
    """
    PDF ã®ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€
    æ”¹è¡Œã‚„ä½™åˆ†ãªç©ºç™½ã‚’æ­£è¦åŒ–ã™ã‚‹
    """
    file_hash = hashlib.md5(file_bytes).hexdigest()
    try:
        with io.BytesIO(file_bytes) as pdf_file:
            extracted_text = extract_text(pdf_file)
            # é€£ç¶šã™ã‚‹ç©ºç™½ã‚„æ”¹è¡Œã‚’1ã¤ã®ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›
            extracted_text = re.sub(r'\s+', ' ', extracted_text).strip()
    except Exception as e:
        st.error(f"PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        extracted_text = ""
    return extracted_text, file_hash

# --- è‰²è‡ªå‹•ç”Ÿæˆãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ©ã‚¤ãƒˆ ---
def generate_color(index):
    hue = (index * 137.508) % 360  # é»„é‡‘æ¯”ã«åŸºã¥ãå‡ç­‰ãªè‰²åˆ†å¸ƒ
    return f"hsl({hue}, 75%, 75%)"

def highlight_text(text, terms):
    for i, term in enumerate(terms):
        color = generate_color(i)
        text = re.sub(f"({re.escape(term)})", rf'<mark style="background-color: {color}">\1</mark>', text, flags=re.IGNORECASE)
    return text

# --- å„ãƒšãƒ¼ã‚¸ã®å†…å®¹ ---

# Home ãƒšãƒ¼ã‚¸
if page == page_list[0]:
    st.title("Home")
    st.title("Patent Analysis App")
    st.write("Welcome to the Patent Analysis Application")
    st.write("This application is for analyzing patent data.")
    st.write("1. Find some patents from [J-PlatPat](https://www.j-platpat.inpit.go.jp/).")
    st.write("2. Download the patent data as CSV files or PDF files.")
    st.write("3. Select the analysis type from the sidebar.")
    st.write("4. Upload the CSV files or PDF files.")

# Patent ãƒšãƒ¼ã‚¸
elif page == page_list[1]:
    st.title("Patent")
    target_col = 'Patent'
    file_summaries = st.file_uploader("Upload CSV files", type='csv', accept_multiple_files=True)
    if file_summaries:
        df_list = []
        for file_summary in file_summaries:
            try:
                df_summary = pd.read_csv(file_summary, encoding='utf-8', encoding_errors='ignore')
                df_list.append(df_summary)
            except Exception as e:
                st.error(f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {file_summary.name} - {e}")
        if df_list:
            df = pd.concat(df_list).reset_index(drop=True)
            df = df.drop_duplicates(subset=['æ–‡çŒ®ç•ªå·','å‡ºé¡˜æ—¥'], keep='first').reset_index(drop=True)
            date_cols = [col for col in df.columns if col.endswith('æ—¥')]
            if not date_cols:
                st.error("æ—¥ä»˜ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            else:
                target_date_col = st.sidebar.selectbox("Select date column", date_cols)
                try:
                    df[target_date_col] = pd.to_datetime(df[target_date_col])
                except Exception as e:
                    st.error(f"æ—¥ä»˜å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
                df['å¹´'] = df[target_date_col].dt.year.astype('int')
                df['å‡ºé¡˜äºº/æ¨©åˆ©è€…'] = df['å‡ºé¡˜äºº/æ¨©åˆ©è€…'].astype('str')
                df['FI'] = df['FI'].astype('str').apply(lambda x: [fi.strip() for fi in x.split(',') if fi.strip()])
                df['FI'] = df['FI'].apply(merge_fi_codes)
                df[['ã‚»ã‚¯ã‚·ãƒ§ãƒ³', 'ã‚¯ãƒ©ã‚¹', 'ã‚µãƒ–ã‚¯ãƒ©ã‚¹', 'ã‚°ãƒ«ãƒ¼ãƒ—']] = df['FI'].apply(lambda x: pd.Series(parse_fi_codes(x)))

                # --- å‡ºé¡˜äººãƒªã‚¹ãƒˆã®æ­£è¦åŒ– ---
                applicant_list = []
                for applicants in df['å‡ºé¡˜äºº/æ¨©åˆ©è€…'].unique():
                    applicants = applicants.replace('ã€',',').replace('ï¼Œ',',').replace(', ',',').replace(',ã€€',',')
                    for applicant in applicants.split(','):
                        if applicant not in applicant_list and len(applicant) > 1:
                            applicant_list.append(applicant)
                applicant_list.sort()
                
                # --- æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ ---
                min_date = df[target_date_col].min().date()
                max_date = df[target_date_col].max().date()
                start_date = st.sidebar.date_input("Start date", min_date, min_value=min_date, max_value=max_date)
                end_date = st.sidebar.date_input("End date", max_date, min_value=min_date, max_value=max_date)
                df_date = df[(df[target_date_col] >= pd.to_datetime(start_date)) & (df[target_date_col] <= pd.to_datetime(end_date))]

                # --- å¹´åˆ¥ãƒ»ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥ä»¶æ•°ã®é›†è¨ˆ ---
                min_year = df_date['å¹´'].min()
                max_year = df_date['å¹´'].max() + 1
                df_year = pd.DataFrame(range(min_year, max_year), columns=['å¹´'])
                df_year['count'] = 0
                for year in range(min_year, max_year):
                    df_year.loc[df_year['å¹´'] == year, 'count'] = df_date[df_date['å¹´'] == year]['æ–‡çŒ®ç•ªå·'].count()
                stage_selector = st.sidebar.multiselect("Select stage type", df['ã‚¹ãƒ†ãƒ¼ã‚¸'].unique(), default=df['ã‚¹ãƒ†ãƒ¼ã‚¸'].unique(), key='stage_selector')
                for stage in stage_selector:
                    for year in range(min_year, max_year):
                        df_year.loc[df_year['å¹´'] == year, f'count_{stage}'] = df_date[(df_date['å¹´'] == year) & (df_date['ã‚¹ãƒ†ãƒ¼ã‚¸'] == stage)]['æ–‡çŒ®ç•ªå·'].count()
                fi_selector = st.sidebar.multiselect("Select FI Section code", df['ã‚»ã‚¯ã‚·ãƒ§ãƒ³'].explode().unique(), default=df['ã‚»ã‚¯ã‚·ãƒ§ãƒ³'].explode().unique(), key='fi_section_selector')
                df_fi = df_date[df_date['ã‚»ã‚¯ã‚·ãƒ§ãƒ³'].apply(lambda x: any(fi in x for fi in fi_selector))]

                # --- å‡ºé¡˜äººåˆ¥é›†è¨ˆ ---
                with st.spinner('Loading data and computing statistics...'):
                    df_applicant_initial = df_date.copy()
                    df_applicant_initial['å‡ºé¡˜äºº/æ¨©åˆ©è€…'] = df_applicant_initial['å‡ºé¡˜äºº/æ¨©åˆ©è€…'].str.split('[ã€ï¼Œ,]')
                    df_applicant_initial = df_applicant_initial.explode('å‡ºé¡˜äºº/æ¨©åˆ©è€…')
                    df_applicant_grouped = df_applicant_initial.groupby('å‡ºé¡˜äºº/æ¨©åˆ©è€…')['æ–‡çŒ®ç•ªå·'].count().reset_index()
                    df_applicant_grouped.rename(columns={'æ–‡çŒ®ç•ªå·': 'ä»¶æ•°'}, inplace=True)
                    df_stage_grouped = df_applicant_initial.groupby(['å‡ºé¡˜äºº/æ¨©åˆ©è€…', 'ã‚¹ãƒ†ãƒ¼ã‚¸'])['æ–‡çŒ®ç•ªå·'].count().unstack(fill_value=0)
                    df_stage_grouped.columns = [f'ã‚¹ãƒ†ãƒ¼ã‚¸_{stage}' for stage in df_stage_grouped.columns]
                    df_stage_grouped.reset_index(inplace=True)
                    df_year_grouped = df_applicant_initial.groupby(['å‡ºé¡˜äºº/æ¨©åˆ©è€…', 'å¹´'])['æ–‡çŒ®ç•ªå·'].count().unstack(fill_value=0)
                    df_year_grouped.columns = [f"{year}å¹´" for year in df_year_grouped.columns]
                    df_year_grouped.reset_index(inplace=True)
                    df_applicant = df_applicant_grouped.merge(df_stage_grouped, on='å‡ºé¡˜äºº/æ¨©åˆ©è€…', how='left')
                    df_applicant = df_applicant.merge(df_year_grouped, on='å‡ºé¡˜äºº/æ¨©åˆ©è€…', how='left')
                    df_applicant.sort_values('ä»¶æ•°', ascending=False, inplace=True)
                
                # --- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ï¼ˆãƒ•ã‚£ãƒ«ã‚¿æ¸ˆãƒ‡ãƒ¼ã‚¿CSVï¼‰ ---
                csv_data = df_date.to_csv(index=False).encode('utf-8')
                st.download_button("Download Filtered Data as CSV", csv_data, "filtered_patents.csv", "text/csv")
                
                applicant = st.sidebar.selectbox("Select applicant", df_applicant['å‡ºé¡˜äºº/æ¨©åˆ©è€…'].unique(), index=0)

                # --- ã‚¿ãƒ–è¡¨ç¤º ---
                tab_overview, tab_applicant, tab_fi, tab_summary = st.tabs(analysis_list)

                # Overview ã‚¿ãƒ–
                with tab_overview:
                    st.header("Overview")
                    st.write("This is an overview analysis page.")
                    st.write(f"The selected date range is from {start_date} to {end_date}.")
                    st.dataframe(df_date)
                    st.header("Visualization")
                    with st.spinner('Visualizing...'):
                        fig1 = go.Figure()
                        fig1.update_layout(title='Patents every year', xaxis_title='Year', yaxis_title='Counts')
                        fig1.add_trace(go.Scatter(
                            x=df_year['å¹´'].values, 
                            y=df_year['count'].values, 
                            mode='lines+markers', 
                            name='Counts every year (All)'
                        ))
                        for stage in stage_selector:
                            fig1.add_trace(go.Scatter(
                                x=df_year['å¹´'].values, 
                                y=df_year[f'count_{stage}'].values, 
                                mode='lines+markers', 
                                name=f'Counts every year ({stage})'
                            ))
                        st.plotly_chart(fig1)

                # Applicant ã‚¿ãƒ–
                with tab_applicant:
                    st.header("Applicant")
                    st.write("This is an applicant analysis page.")
                    st.write(df_applicant)
                    num_applicant = st.slider("Number of applicants", 1, len(df_applicant), 50)
                    st.header("Visualization")
                    with st.spinner('Visualizing...'):
                        fig2 = go.Figure()
                        for col in df_applicant.columns:
                            if col.startswith("ã‚¹ãƒ†ãƒ¼ã‚¸_"):
                                fig2.add_trace(go.Bar(
                                    x=df_applicant[col].values[:num_applicant],
                                    y=df_applicant['å‡ºé¡˜äºº/æ¨©åˆ©è€…'].values[:num_applicant],
                                    name=col.replace("ã‚¹ãƒ†ãƒ¼ã‚¸_", ""),
                                    orientation='h'
                                ))
                        fig2.update_layout(
                            title='Patents per Applicant',
                            height=1200,
                            width=900,
                            xaxis_title='Counts',
                            yaxis_title='Applicants',
                            yaxis=dict(autorange="reversed"),
                            barmode='stack'
                        )
                        st.plotly_chart(fig2)
                    
                    with st.spinner("Visualizing applicant trends..."):
                        fig3 = go.Figure()
                        try:
                            applicant_year_data = df_year_grouped[df_year_grouped['å‡ºé¡˜äºº/æ¨©åˆ©è€…'] == applicant]
                            if not applicant_year_data.empty:
                                years = [int(col.replace('å¹´', '')) for col in applicant_year_data.columns if col.endswith('å¹´')]
                                counts = [applicant_year_data[col].values[0] for col in applicant_year_data.columns if col.endswith('å¹´')]
                                fig3.add_trace(go.Scatter(
                                    x=years,
                                    y=counts,
                                    mode='lines+markers',
                                    name='Patents per Year'
                                ))
                                fig3.update_layout(
                                    title=f'Patents per Applicant ({applicant})',
                                    xaxis_title='Year',
                                    yaxis_title='Counts'
                                )
                                st.plotly_chart(fig3)
                        except Exception as e:
                            st.error(f"å¹´æ¬¡ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                    
                    # ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆã«ã‚ˆã‚‹å¹´æ¬¡ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–
                    df_applicant_melted = df_applicant[:num_applicant].melt(
                        id_vars=['å‡ºé¡˜äºº/æ¨©åˆ©è€…'],
                        value_vars=[col for col in df_applicant.columns if col.endswith('å¹´')],
                        var_name='å¹´',
                        value_name='å‡ºé¡˜ä»¶æ•°'
                    )
                    df_applicant_melted['å¹´'] = df_applicant_melted['å¹´'].str.replace('å¹´', '').astype(int)
                    with st.spinner('Visualizing bubble chart...'):
                        fig_bubble = go.Figure()
                        for app in df_applicant_melted['å‡ºé¡˜äºº/æ¨©åˆ©è€…'].unique():
                            df_subset = df_applicant_melted[df_applicant_melted['å‡ºé¡˜äºº/æ¨©åˆ©è€…'] == app]
                            fig_bubble.add_trace(go.Scatter(
                                x=df_subset['å¹´'],
                                y=[app] * len(df_subset),
                                mode='markers',
                                marker=dict(
                                    size=df_subset['å‡ºé¡˜ä»¶æ•°'] * 2,
                                    opacity=0.6,
                                    line=dict(width=1, color='black')
                                ),
                                name=app
                            ))
                        fig_bubble.update_layout(
                            title="Applicant-wise Annual Patent Applications (Bubble Chart)",
                            xaxis_title="Year",
                            yaxis_title="Applicants",
                            yaxis=dict(autorange="reversed"),
                            showlegend=True,
                            height=1600,
                            width=1200
                        )
                        st.plotly_chart(fig_bubble)
                    
                    # --- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æã®è¿½åŠ æ©Ÿèƒ½ ---
                    st.subheader("Clustering Analysis on Applicants")
                    num_clusters = st.slider("Select number of clusters", 2, 10, 3, key="clusters_slider")
                    clustering_features = df_applicant.filter(regex='^(ä»¶æ•°|ã‚¹ãƒ†ãƒ¼ã‚¸_)').fillna(0)
                    if not clustering_features.empty:
                        kmeans = KMeans(n_clusters=num_clusters, random_state=42)
                        df_applicant['cluster'] = kmeans.fit_predict(clustering_features)
                        st.write("Clustering results:")
                        st.dataframe(df_applicant[['å‡ºé¡˜äºº/æ¨©åˆ©è€…', 'ä»¶æ•°', 'cluster']])
                        cluster_counts = df_applicant['cluster'].value_counts().sort_index()
                        fig_cluster = go.Figure(go.Bar(
                            x=cluster_counts.index.astype(str),
                            y=cluster_counts.values
                        ))
                        fig_cluster.update_layout(title="Number of Applicants in each Cluster", xaxis_title="Cluster", yaxis_title="Count")
                        st.plotly_chart(fig_cluster)

                # FI ã‚¿ãƒ–
                with tab_fi:
                    st.header("FI")
                    fi_reference_url = 'https://www.j-platpat.inpit.go.jp/cache/classify/patent/PMGS_HTML/jpp/FI/ja/fiSection/fiSection.html'
                    st.write(f"Please refer to the following URL for the FI classification: [J-PlatPat:FIã‚»ã‚¯ã‚·ãƒ§ãƒ³/åºƒåŸŸãƒ•ã‚¡ã‚»ãƒƒãƒˆé¸æŠğŸ“Œ]({fi_reference_url})")
                    fig4 = go.Figure()
                    fig4.add_trace(go.Pie(
                        labels=df_fi['ã‚»ã‚¯ã‚·ãƒ§ãƒ³'].explode().value_counts().index,
                        values=df_fi['ã‚»ã‚¯ã‚·ãƒ§ãƒ³'].explode().value_counts().values,
                        rotation=0,
                        hole=0.3,
                        title='FI Section',
                        textinfo='label+percent',
                    ))
                    fig4.update_layout(title='FI Section', height=800, width=800)
                    st.plotly_chart(fig4)
                    fig5 = go.Figure()
                    fig5.add_trace(go.Bar(
                        x=df_fi['ã‚¯ãƒ©ã‚¹'].explode().value_counts().index,
                        y=df_fi['ã‚¯ãƒ©ã‚¹'].explode().value_counts().values,
                        name='FI Class',
                    ))
                    fig5.update_layout(title='FI Class', height=600, width=1200, xaxis_title='FI Class', yaxis_title='Counts')
                    st.plotly_chart(fig5)

                # Summary ã‚¿ãƒ–
                with tab_summary:
                    st.header("Summary")
                    if 'è¦ç´„' not in df.columns.to_list():
                        st.write("There is no summary data in the uploaded file.")
                    else:
                        st.write(df_date[['æ–‡çŒ®ç•ªå·','å‡ºé¡˜äºº/æ¨©åˆ©è€…','è¦ç´„']])
    else:
        st.info("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

# Claim ãƒšãƒ¼ã‚¸
elif page == page_list[2]:
    st.title("Claim")
    target_col = 'Claim'
    file_pdfs = st.file_uploader("Upload PDF files", type='pdf', accept_multiple_files=True)
    total_files = len(file_pdfs)
    pdf_name_list = []
    pdf_text_dict = {}
    if file_pdfs:
        extract_bar = st.progress(0)
        with st.spinner('Extracting PDF texts...'):
            for i, file_pdf in enumerate(file_pdfs):
                try:
                    file_bytes = file_pdf.read()
                    extracted_text, file_hash = extract_text_from_pdf(file_bytes)
                    pdf_name_list.append(file_pdf.name)
                    pdf_text_dict[file_hash] = extracted_text
                except Exception as e:
                    st.error(f"{file_pdf.name} ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                extract_bar.progress((i+1)/total_files)
                time.sleep(0.1)
            extract_bar.empty()
    # --- æŠ½å‡ºçµæœã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ---
    if pdf_text_dict:
        pdf_df = pd.DataFrame({
            "file_name": pdf_name_list,
            "extracted_text": list(pdf_text_dict.values())
        })
        csv_pdf = pdf_df.to_csv(index=False).encode('utf-8')
        st.download_button("Download Extracted PDF Texts as CSV", csv_pdf, "extracted_pdf_texts.csv", "text/csv")
        
    search_query = st.sidebar.text_input("Enter keywords (comma separated)", "")
    search_terms = [term.strip() for term in search_query.split(',') if term.strip()]
    if pdf_text_dict:
        display_bar = st.progress(0)
        st.write("Search terms: ", search_terms)
        with st.spinner('Processing PDFs...'):
            for i, (name, text) in enumerate(zip(pdf_name_list, pdf_text_dict.values())):
                with st.expander(f"{i+1}/{total_files}: {name}", expanded=True):
                    st.header(f"{i+1}/{total_files}: {name}")
                    text = re.sub(r'\s+', ' ', text)
                    try:
                        subject = text.split('ã€èª²é¡Œã€‘')[1].split('ã€è§£æ±ºæ‰‹æ®µã€‘')[0].strip()
                        subject = highlight_text(subject, search_terms)
                        st.markdown(f'**ã€èª²é¡Œã€‘**<br>{subject}', unsafe_allow_html=True)
                    except IndexError:
                        st.warning("ã€èª²é¡Œã€‘ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    try:
                        solution = text.split('ã€è§£æ±ºæ‰‹æ®µã€‘')[1].split('ã€é¸æŠå›³ã€‘')[0].strip()
                        solution = highlight_text(solution, search_terms)
                        st.markdown(f'**ã€è§£æ±ºæ‰‹æ®µã€‘**<br>{solution}', unsafe_allow_html=True)
                    except IndexError:
                        st.warning("ã€è§£æ±ºæ‰‹æ®µã€‘ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    try:
                        figure = text.split('ã€é¸æŠå›³ã€‘')[1].split('ã€ç‰¹è¨±è«‹æ±‚ã®ç¯„å›²ã€‘')[0].strip()
                        st.markdown(f'**ã€é¸æŠå›³ã€‘**<br>{figure}', unsafe_allow_html=True)
                    except IndexError:
                        st.warning("ã€é¸æŠå›³ã€‘ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    try:
                        claims = text.split('ã€ç‰¹è¨±è«‹æ±‚ã®ç¯„å›²ã€‘')[1].split('ã€ç™ºæ˜ã®è©³ç´°ãªèª¬æ˜ã€‘')[0].strip()
                        claims_list = claims.split('ã€è«‹æ±‚é …')
                        st.markdown(f'**ã€ç‰¹è¨±è«‹æ±‚ã®ç¯„å›²ã€‘**<br>', unsafe_allow_html=True)
                        for claim in claims_list:
                            if claim:
                                claim_text = highlight_text('ã€è«‹æ±‚é …' + claim, search_terms)
                                st.markdown(claim_text, unsafe_allow_html=True)
                    except IndexError:
                        st.warning("ã€ç‰¹è¨±è«‹æ±‚ã®ç¯„å›²ã€‘ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    display_bar.progress((i+1)/total_files)
                    time.sleep(0.1)
            display_bar.empty()
        st.header("EOF")
    else:
        st.info("PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

# Others ãƒšãƒ¼ã‚¸
elif page == page_list[3]:
    st.title("Others")
    st.write("This is a page for other analysis.")
